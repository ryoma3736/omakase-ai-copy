"use client";

import { useState, useRef, useEffect, useCallback } from "react";
import { Mic, MicOff, Send, Volume2, VolumeX, ShoppingBag, Loader2 } from "lucide-react";

/**
 * Omakase AI Demo - ãƒãƒ£ãƒƒãƒˆï¼†éŸ³å£°AIãƒ‡ãƒ¢ãƒšãƒ¼ã‚¸
 * èªè¨¼ä¸è¦ã§ç›´æ¥AIã¨ä¼šè©±ã§ãã‚‹
 */
// æŒ¨æ‹¶ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
const GREETING = "ã“ã‚“ã«ã¡ã¯ï¼ãŠã¾ã‹ã›AIã§ã™ã€‚å•†å“ã®ã”è³ªå•ã‚„ãŠã™ã™ã‚ãªã©ã€ä½•ã§ã‚‚ãŠæ°—è»½ã«ã©ã†ãï¼";

export default function DemoPage() {
  const [messages, setMessages] = useState<{ role: "user" | "assistant"; content: string }[]>([
    { role: "assistant", content: GREETING }
  ]);
  const [input, setInput] = useState("");
  const [isLoading, setIsLoading] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [isTTSGenerating, setIsTTSGenerating] = useState(false);
  const [lastAssistantMessage, setLastAssistantMessage] = useState(GREETING);
  const messagesEndRef = useRef<HTMLDivElement>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const recognitionRef = useRef<any>(null);

  // äº‹å‰ç”Ÿæˆæ¸ˆã¿æŒ¨æ‹¶éŸ³å£°ã‚­ãƒ£ãƒƒã‚·ãƒ¥
  const greetingAudioRef = useRef<string | null>(null);

  const scrollToBottom = () => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  };

  useEffect(() => {
    scrollToBottom();
  }, [messages]);

  // æŒ¨æ‹¶éŸ³å£°ã‚’äº‹å‰ç”Ÿæˆï¼ˆãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰æ™‚ï¼‰
  useEffect(() => {
    const preloadGreeting = async () => {
      try {
        const response = await fetch("/api/tts", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ text: GREETING, voice: "Kore" }),
        });
        const data = await response.json();
        if (data.success && data.audio) {
          greetingAudioRef.current = data.audio;
          console.log("Greeting audio preloaded");
        }
      } catch (error) {
        console.error("Failed to preload greeting:", error);
      }
    };
    preloadGreeting();
  }, []);

  // ç„¡éŸ³ã‚¿ã‚¤ãƒãƒ¼
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const finalTranscriptRef = useRef("");

  // éŸ³å£°èªè­˜ - ãƒˆã‚°ãƒ«æ–¹å¼ï¼ˆæŠ¼ã™ã¾ã§åˆ‡ã‚Œãªã„ï¼‰
  const toggleListening = useCallback(() => {
    // æ—¢ã«èªè­˜ä¸­ãªã‚‰åœæ­¢ã—ã¦é€ä¿¡
    if (isListening && recognitionRef.current) {
      recognitionRef.current.stop();
      return;
    }

    if (!("webkitSpeechRecognition" in window || "SpeechRecognition" in window)) {
      alert("ãŠä½¿ã„ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°èªè­˜ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“");
      return;
    }

    // TTSå†ç”Ÿä¸­ã¯åœæ­¢
    if (isSpeaking) {
      stopSpeaking();
    }

    const SpeechRecognition = (window as any).webkitSpeechRecognition || (window as any).SpeechRecognition;
    const recognition = new SpeechRecognition();
    recognition.lang = "ja-JP";
    recognition.continuous = true;
    recognition.interimResults = true;

    finalTranscriptRef.current = "";

    recognition.onstart = () => setIsListening(true);

    recognition.onend = () => {
      // å…¥åŠ›ãŒã‚ã‚Œã°é€ä¿¡
      if (finalTranscriptRef.current.trim()) {
        setInput(finalTranscriptRef.current);
        setTimeout(() => {
          const sendBtn = document.querySelector('[data-send-btn]') as HTMLButtonElement;
          sendBtn?.click();
        }, 100);
      }
      setIsListening(false);
      recognitionRef.current = null;
    };

    recognition.onresult = (event: any) => {
      let interimTranscript = "";
      let finalTranscript = "";

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      if (finalTranscript) {
        finalTranscriptRef.current += finalTranscript;
      }

      setInput(finalTranscriptRef.current + interimTranscript);
    };

    recognition.onerror = (event: any) => {
      // ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‹æ‰‹ã«åˆ‡ã‚‰ãªã„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒåœæ­¢ãƒœã‚¿ãƒ³æŠ¼ã™ã¾ã§ç¶™ç¶š
      console.error("Speech recognition error:", event.error);
      if (event.error !== "no-speech" && event.error !== "aborted") {
        // è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ä»¥å¤–ã¯å†é–‹
        if (recognitionRef.current) {
          try {
            recognition.start();
          } catch (e) {
            // æ—¢ã«é–‹å§‹æ¸ˆã¿ã®å ´åˆã¯ç„¡è¦–
          }
        }
      }
    };

    recognitionRef.current = recognition;
    recognition.start();
  }, [isListening, isSpeaking]);

  // éŸ³å£°åˆæˆ (Gemini 2.5 TTS) - ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œ
  const speak = useCallback(async (text: string) => {
    setLastAssistantMessage(text);

    // æŒ¨æ‹¶ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ãªã‚‰å³å†ç”Ÿ
    if (text === GREETING && greetingAudioRef.current) {
      setIsSpeaking(true);
      const audioBlob = new Blob(
        [Uint8Array.from(atob(greetingAudioRef.current), c => c.charCodeAt(0))],
        { type: "audio/wav" }
      );
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      audioRef.current = audio;
      audio.onended = () => {
        setIsSpeaking(false);
        URL.revokeObjectURL(audioUrl);
        audioRef.current = null;
      };
      await audio.play();
      return;
    }

    setIsTTSGenerating(true);

    try {
      const response = await fetch("/api/tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text, voice: "Kore" }),
      });

      const data = await response.json();
      setIsTTSGenerating(false);

      if (data.success && data.audio) {
        setIsSpeaking(true);
        // Base64 audio ã‚’å†ç”Ÿ
        const audioBlob = new Blob(
          [Uint8Array.from(atob(data.audio), c => c.charCodeAt(0))],
          { type: data.mimeType || "audio/wav" }
        );
        const audioUrl = URL.createObjectURL(audioBlob);
        const audio = new Audio(audioUrl);
        audioRef.current = audio;
        audio.onended = () => {
          setIsSpeaking(false);
          URL.revokeObjectURL(audioUrl);
          audioRef.current = null;
        };
        audio.onerror = () => {
          setIsSpeaking(false);
          URL.revokeObjectURL(audioUrl);
          audioRef.current = null;
          // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
          fallbackSpeak(text);
        };
        await audio.play();
      } else {
        fallbackSpeak(text);
      }
    } catch (error) {
      console.error("TTS error:", error);
      setIsTTSGenerating(false);
      fallbackSpeak(text);
    }
  }, []);

  // Web Speech API ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
  const fallbackSpeak = useCallback((text: string) => {
    if ("speechSynthesis" in window) {
      setIsSpeaking(true);
      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = "ja-JP";
      utterance.rate = 1.1;
      utterance.onend = () => setIsSpeaking(false);
      utterance.onerror = () => setIsSpeaking(false);
      window.speechSynthesis.speak(utterance);
    }
  }, []);

  const stopSpeaking = () => {
    // Stop Gemini TTS audio
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current = null;
    }
    // Stop Web Speech API (fallback)
    if ("speechSynthesis" in window) {
      window.speechSynthesis.cancel();
    }
    setIsSpeaking(false);
  };

  // TTSéŸ³å£°ã‚­ãƒ¥ãƒ¼ç®¡ç†
  const audioQueueRef = useRef<{ audio: string; text: string }[]>([]);
  const isPlayingQueueRef = useRef(false);

  const playNextInQueue = useCallback(async () => {
    if (isPlayingQueueRef.current || audioQueueRef.current.length === 0) return;

    isPlayingQueueRef.current = true;
    const item = audioQueueRef.current.shift()!;

    try {
      setIsSpeaking(true);
      const audioBlob = new Blob(
        [Uint8Array.from(atob(item.audio), c => c.charCodeAt(0))],
        { type: "audio/wav" }
      );
      const audioUrl = URL.createObjectURL(audioBlob);
      const audio = new Audio(audioUrl);
      audioRef.current = audio;

      await new Promise<void>((resolve) => {
        audio.onended = () => {
          URL.revokeObjectURL(audioUrl);
          resolve();
        };
        audio.onerror = () => {
          URL.revokeObjectURL(audioUrl);
          resolve();
        };
        audio.play();
      });
    } catch (error) {
      console.error("Queue playback error:", error);
    }

    isPlayingQueueRef.current = false;

    // æ¬¡ã®ã‚­ãƒ¥ãƒ¼ã‚’å†ç”Ÿ
    if (audioQueueRef.current.length > 0) {
      playNextInQueue();
    } else {
      setIsSpeaking(false);
      audioRef.current = null;
    }
  }, []);

  // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§TTSç”Ÿæˆã—ã¦å³åº§ã«ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
  const generateAndQueueTTS = useCallback(async (text: string) => {
    try {
      const response = await fetch("/api/tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ text, voice: "Kore" }),
      });

      const data = await response.json();

      if (data.success && data.audio) {
        audioQueueRef.current.push({ audio: data.audio, text });
        // ã‚­ãƒ¥ãƒ¼ãŒç©ºãªã‚‰å†ç”Ÿé–‹å§‹
        if (!isPlayingQueueRef.current) {
          playNextInQueue();
        }
      }
    } catch (error) {
      console.error("TTS queue error:", error);
    }
  }, [playNextInQueue]);

  // ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ï¼ˆè¶…é«˜é€Ÿçµ±åˆAPIä½¿ç”¨ï¼‰
  const sendMessage = async () => {
    if (!input.trim() || isLoading) return;

    const userMessage = input.trim();
    setInput("");
    setMessages(prev => [...prev, { role: "user", content: userMessage }]);
    setIsLoading(true);
    setIsTTSGenerating(true);

    // ã‚­ãƒ¥ãƒ¼ã‚’ã‚¯ãƒªã‚¢
    audioQueueRef.current = [];

    // ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç©ºã§è¿½åŠ 
    setMessages(prev => [...prev, { role: "assistant", content: "" }]);

    const startTime = performance.now();

    try {
      // çµ±åˆChat+TTS APIã‚’ä½¿ç”¨ï¼ˆChatå¿œç­”ã¨TTSç”ŸæˆãŒä¸¦åˆ—å®Ÿè¡Œï¼‰
      const response = await fetch("/api/chat-with-tts", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          message: userMessage,
          history: messages.filter(m => m.role !== "assistant" || m.content !== ""),
          voice: "Kore",
        }),
      });

      const reader = response.body?.getReader();
      if (!reader) throw new Error("No reader");

      const decoder = new TextDecoder();
      let fullResponse = "";

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        const lines = chunk.split("\n");

        for (const line of lines) {
          if (!line.startsWith("data: ")) continue;

          try {
            const data = JSON.parse(line.slice(6));

            if (data.type === "text") {
              // éƒ¨åˆ†ãƒ†ã‚­ã‚¹ãƒˆ - UIè¡¨ç¤ºã‚’å³åº§ã«æ›´æ–°
              fullResponse += data.text;
              setMessages(prev => {
                const newMsgs = [...prev];
                newMsgs[newMsgs.length - 1] = { role: "assistant", content: fullResponse };
                return newMsgs;
              });
              console.log(`ğŸ“ Text received in ${data.elapsed}ms`);
            } else if (data.type === "audio") {
              // éŸ³å£°ãƒ‡ãƒ¼ã‚¿ - å³åº§ã«å†ç”Ÿé–‹å§‹
              console.log(`ğŸ”Š Audio received in ${data.elapsed}ms`);
              setIsTTSGenerating(false);

              // å³åº§ã«å†ç”Ÿ
              setIsSpeaking(true);
              const audioBlob = new Blob(
                [Uint8Array.from(atob(data.audio), c => c.charCodeAt(0))],
                { type: data.mimeType || "audio/wav" }
              );
              const audioUrl = URL.createObjectURL(audioBlob);
              const audio = new Audio(audioUrl);
              audioRef.current = audio;
              audio.onended = () => {
                setIsSpeaking(false);
                URL.revokeObjectURL(audioUrl);
                audioRef.current = null;
              };
              audio.onerror = () => {
                setIsSpeaking(false);
                URL.revokeObjectURL(audioUrl);
                audioRef.current = null;
              };
              await audio.play();
            } else if (data.type === "done") {
              setLastAssistantMessage(data.fullText || fullResponse);
              const totalTime = performance.now() - startTime;
              console.log(`âœ… Total response time: ${totalTime.toFixed(0)}ms (Server: ${data.totalElapsed}ms)`);
            } else if (data.type === "error") {
              console.error("Stream error:", data.message);
            }
          } catch (e) {
            // JSON parse error - skip
          }
        }
      }

      if (!fullResponse) {
        setMessages(prev => {
          const newMsgs = [...prev];
          newMsgs[newMsgs.length - 1] = { role: "assistant", content: "ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚" };
          return newMsgs;
        });
      }
    } catch (error) {
      console.error("Chat error:", error);
      setMessages(prev => {
        const newMsgs = [...prev];
        newMsgs[newMsgs.length - 1] = { role: "assistant", content: "æ¥ç¶šã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚" };
        return newMsgs;
      });
    } finally {
      setIsLoading(false);
      setIsTTSGenerating(false);
    }
  };

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === "Enter" && !e.shiftKey) {
      e.preventDefault();
      sendMessage();
    }
  };

  // ã‚µãƒ³ãƒ—ãƒ«è³ªå•
  const sampleQuestions = [
    "ãŠã™ã™ã‚ã®å•†å“ã‚’æ•™ãˆã¦",
    "ä¸€ç•ªäººæ°—ã®å•†å“ã¯ï¼Ÿ",
    "äºˆç®—1ä¸‡å††ã§ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆã‚’æ¢ã—ã¦ã‚‹",
    "è¿”å“ãƒãƒªã‚·ãƒ¼ã‚’æ•™ãˆã¦",
  ];

  return (
    <div className="min-h-screen bg-gradient-to-br from-gray-900 via-black to-gray-900">
      {/* Header */}
      <header className="bg-black/50 backdrop-blur-md border-b border-white/10 px-6 py-4">
        <div className="max-w-4xl mx-auto flex items-center justify-between">
          <div className="flex items-center gap-3">
            <div className="w-10 h-10 bg-gradient-to-br from-cyan-400 to-blue-500 rounded-xl flex items-center justify-center">
              <ShoppingBag className="w-5 h-5 text-white" />
            </div>
            <div>
              <h1 className="text-xl font-bold text-white">Omakase AI Demo</h1>
              <p className="text-xs text-gray-400">Voice + Chat AI Agent</p>
            </div>
          </div>
          <div className="flex items-center gap-2 text-xs text-gray-400">
            <span className="w-2 h-2 bg-green-500 rounded-full animate-pulse"></span>
            Powered by Gemini
          </div>
        </div>
      </header>

      {/* Main Chat Area */}
      <main className="max-w-4xl mx-auto p-4 pb-32">
        {/* Sample Questions */}
        <div className="mb-6">
          <p className="text-gray-500 text-sm mb-2">ã‚¯ã‚¤ãƒƒã‚¯è³ªå•:</p>
          <div className="flex flex-wrap gap-2">
            {sampleQuestions.map((q, i) => (
              <button
                key={i}
                onClick={() => setInput(q)}
                className="bg-white/5 hover:bg-white/10 border border-white/10 rounded-full px-4 py-2 text-sm text-gray-300 transition-colors"
              >
                {q}
              </button>
            ))}
          </div>
        </div>

        {/* Messages */}
        <div className="space-y-4">
          {messages.map((msg, i) => (
            <div
              key={i}
              className={`flex ${msg.role === "user" ? "justify-end" : "justify-start"}`}
            >
              <div
                className={`max-w-[80%] rounded-2xl px-4 py-3 ${
                  msg.role === "user"
                    ? "bg-cyan-500 text-black"
                    : "bg-white/10 text-white"
                }`}
              >
                <p className="whitespace-pre-wrap">{msg.content}</p>
              </div>
            </div>
          ))}
          {isLoading && (
            <div className="flex justify-start">
              <div className="bg-white/10 rounded-2xl px-4 py-3">
                <div className="flex items-center gap-2">
                  <div className="w-2 h-2 bg-cyan-400 rounded-full animate-bounce"></div>
                  <div className="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style={{ animationDelay: "0.1s" }}></div>
                  <div className="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style={{ animationDelay: "0.2s" }}></div>
                </div>
              </div>
            </div>
          )}
          <div ref={messagesEndRef} />
        </div>
      </main>

      {/* Input Area */}
      <div className="fixed bottom-0 left-0 right-0 bg-black/80 backdrop-blur-md border-t border-white/10 p-4">
        <div className="max-w-4xl mx-auto">
          <div className="flex items-center gap-3">
            {/* Voice Button - ãƒˆã‚°ãƒ«å¼ï¼ˆ1å›æŠ¼ã—ã¦é–‹å§‹ã€ã‚‚ã†1å›æŠ¼ã—ã¦åœæ­¢ï¼†é€ä¿¡ï¼‰ */}
            <button
              onClick={toggleListening}
              disabled={isTTSGenerating || isLoading}
              className={`w-14 h-14 rounded-full flex items-center justify-center transition-all ${
                isListening
                  ? "bg-red-500 ring-4 ring-red-500/50 scale-110"
                  : isTTSGenerating || isLoading
                  ? "bg-white/5 cursor-not-allowed"
                  : "bg-cyan-500 hover:bg-cyan-400"
              }`}
              title={isListening ? "ğŸ›‘ ã‚¯ãƒªãƒƒã‚¯ã§åœæ­¢ï¼†é€ä¿¡" : "ğŸ¤ ã‚¯ãƒªãƒƒã‚¯ã§éŸ³å£°å…¥åŠ›é–‹å§‹"}
            >
              {isListening ? (
                <MicOff className="w-6 h-6 text-white" />
              ) : (
                <Mic className="w-6 h-6 text-black" />
              )}
            </button>

            {/* Text Input */}
            <div className="flex-1 relative">
              <input
                type="text"
                value={input}
                onChange={(e) => setInput(e.target.value)}
                onKeyPress={handleKeyPress}
                placeholder={isListening ? "è©±ã—ã¦ãã ã•ã„..." : "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›..."}
                disabled={isListening}
                className={`w-full bg-white/10 border rounded-full px-5 py-3 text-white placeholder-gray-500 focus:outline-none transition-all ${
                  isListening
                    ? "border-red-500/50 bg-red-500/10"
                    : "border-white/10 focus:border-cyan-500"
                }`}
              />
            </div>

            {/* Send Button */}
            <button
              data-send-btn
              onClick={sendMessage}
              disabled={!input.trim() || isLoading || isListening}
              className={`w-12 h-12 rounded-full flex items-center justify-center transition-all ${
                isLoading
                  ? "bg-cyan-500/50"
                  : !input.trim() || isListening
                  ? "bg-white/10 opacity-50"
                  : "bg-cyan-500 hover:bg-cyan-400"
              }`}
              title="é€ä¿¡"
            >
              {isLoading ? (
                <Loader2 className="w-5 h-5 text-black animate-spin" />
              ) : (
                <Send className="w-5 h-5 text-black" />
              )}
            </button>

            {/* Speaker Button - TTSçŠ¶æ…‹è¡¨ç¤º + ãƒªãƒ—ãƒ¬ã‚¤æ©Ÿèƒ½ */}
            <button
              onClick={isSpeaking ? stopSpeaking : lastAssistantMessage ? () => speak(lastAssistantMessage) : undefined}
              disabled={isTTSGenerating && !isSpeaking}
              className={`w-12 h-12 rounded-full flex items-center justify-center transition-all ${
                isTTSGenerating
                  ? "bg-yellow-500 animate-pulse"
                  : isSpeaking
                  ? "bg-cyan-500 animate-pulse"
                  : lastAssistantMessage
                  ? "bg-white/10 hover:bg-white/20"
                  : "bg-white/5 cursor-not-allowed"
              }`}
              title={
                isTTSGenerating
                  ? "éŸ³å£°ç”Ÿæˆä¸­..."
                  : isSpeaking
                  ? "åœæ­¢"
                  : lastAssistantMessage
                  ? "å†ç”Ÿ"
                  : "éŸ³å£°ãªã—"
              }
            >
              {isTTSGenerating ? (
                <Loader2 className="w-5 h-5 text-black animate-spin" />
              ) : isSpeaking ? (
                <Volume2 className="w-5 h-5 text-black" />
              ) : (
                <Volume2 className="w-5 h-5 text-gray-400" />
              )}
            </button>
          </div>

          <p className="text-center text-gray-500 text-xs mt-3">
            {isListening
              ? "ğŸ”´ éŒ²éŸ³ä¸­... ã‚‚ã†ä¸€åº¦æŠ¼ã™ã¨åœæ­¢ï¼†é€ä¿¡"
              : "ğŸ¤ ãƒã‚¤ã‚¯ã‚’æŠ¼ã—ã¦è©±ã™ â†’ ã‚‚ã†ä¸€åº¦æŠ¼ã—ã¦é€ä¿¡"}
          </p>
        </div>
      </div>
    </div>
  );
}
